## Codebase Patterns
- Use argon2 for API key hashing (via argon2-cffi package)
- Database queries use parameterized queries via psycopg2 with %s placeholders
- Flask routes are registered via exposeRoutes() in Medic/Core/routes.py
- Unit tests use pytest with fixtures defined in tests/conftest.py
- Log levels use Medic.Helpers.logSettings.logSetup()
- Return JSON responses as tuple (json_string, status_code)
- Database returns JSON string when show_columns=True in query_db()
- Health endpoints: /health, /health/live, /health/ready, /v1/healthcheck/network
- API keys stored in medic.api_keys table with key_hash, scopes array, expires_at
- Prometheus metrics: define Counter/Gauge/Histogram in Medic/Core/metrics.py, use record_*() helpers to increment
- Rate limiting: Use InMemoryRateLimiter for single-instance, sliding window algorithm in Medic/Core/rate_limiter.py
- Rate limit middleware: @rate_limit() decorator in Medic/Core/rate_limit_middleware.py, apply AFTER @authenticate_request()
- Rate limit headers: X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Reset (plus Retry-After when blocked)
- Webhook delivery: Use WebhookDeliveryService in Medic/Core/webhook_delivery.py with deliver_webhook() convenience function
- HTTP retries: Use exponential backoff with configurable delays (default: 1s, 5s, 30s) for webhook delivery
- Alert routing: Use get_slack_channel_for_service(service_id) from Medic/Core/alert_routing.py to get team channel with fallback to default
- Notification targets: medic.notification_targets table stores type-specific config in JSONB, ordered by priority (lower = higher priority)
- Flexible alert routing: Use route_alert(service_id, payload, mode) with NotificationMode.NOTIFY_ALL or NOTIFY_UNTIL_SUCCESS
- Notification sender: Pass custom sender callback to route_alert() or use default_notification_sender() placeholder
- Working hours: medic.schedules table stores timezone (IANA format) and hours (JSONB with day/hour ranges). Services link via schedule_id FK.
- Working hours evaluation: Use is_within_working_hours(schedule, check_time) from Medic/Core/working_hours.py. Uses zoneinfo for DST-aware timezone handling.
- Service working hours: Use is_service_within_working_hours(service_id) which returns (bool, schedule_name) tuple. Returns (True, None) if no schedule (always in hours).
- Period detection: Use get_current_period(schedule) or get_service_current_period(service_id) to get "during_hours" or "after_hours" string.
- Working hours routing: Use route_alert_with_schedule(service_id, payload, mode, sender, check_time) for working hours-aware alert routing. Automatically uses correct targets based on service schedule.
- Notification target periods: notification_targets.period column values: 'always' (default), 'during_hours', 'after_hours'. Targets with 'always' included in all routing.
- Maintenance windows: Use is_service_in_maintenance(service_id, check_time) from Medic/Core/maintenance_windows.py. Returns True if service is in any active window.
- Recurring maintenance: Cron expressions evaluated via croniter library. Window duration = end_time - start_time from original definition.
- Maintenance window queries: Empty service_ids means ALL services. Query: WHERE service_ids = '{}' OR %s = ANY(service_ids)
- Heartbeat status: HeartbeatStatus enum in Medic/Helpers/heartbeat.py defines valid values (UP, DOWN, STARTED, COMPLETED, FAILED). Use is_valid() to validate, is_job_status() to check job-related.
- Job correlation: Heartbeat objects support optional run_id parameter for correlating STARTED/COMPLETED/FAILED events. Use queryHeartbeatsByRunId(run_id) to find correlated events.
- V2 job signal endpoints: POST /v2/heartbeat/:id/start|complete|fail accepts service_id as path param, optional run_id in JSON body. Uses _record_job_signal() helper.
- Job runs tracking: medic.job_runs table stores duration statistics. Use record_job_start() and record_job_completion() from Medic/Core/job_runs.py.
- Duration calculation: Duration in milliseconds calculated as completed_at - started_at. Stored in job_runs.duration_ms column.
- Job run queries: get_completed_runs_for_service(service_id, limit=100) returns runs ordered by completion time for stats. get_stale_runs() finds jobs that started but never completed.
- Max duration: services.max_duration_ms column defines threshold for duration alerts (to be used by US-023).
- Duration statistics: GET /v2/services/:id/stats returns DurationStatistics (avg, p50, p95, p99, min, max). Returns empty stats (null values) if fewer than 5 runs. Uses get_duration_statistics() from Medic/Core/job_runs.py.
- Duration alerts: Use check_duration_threshold(job_run) to check if completed job exceeded max_duration_ms. Returns DurationAlert if exceeded, None otherwise.
- Stale job detection: Use get_stale_runs_exceeding_max_duration() to find STARTED jobs that haven't completed within max_duration_ms. Returns list of DurationAlert objects.
- Duration alert metrics: medic_duration_alerts_total{alert_type} (exceeded, stale), medic_stale_jobs_current gauge. Use record_duration_alert() and update_stale_jobs_count() helpers.
- Stale run marking: Use mark_stale_run_alerted(service_id, run_id) to mark job as STALE_ALERTED and prevent duplicate alerts.
- Grace periods: services.grace_period_seconds column delays alerting after missed heartbeat. Check in monitor.py with: required_delay = (interval * 60) + grace_period_seconds; if time_since_last < required_delay: continue.
- Playbooks schema: medic.playbooks stores YAML definitions (playbook_id, name, description, yaml_content, version). medic.playbook_triggers defines conditions for triggering playbooks.
- Playbook triggers: service_pattern uses glob syntax (e.g., "worker-*", "*"), consecutive_failures threshold, enabled boolean. Triggers link to playbooks via FK with CASCADE delete.

---

## 2026-02-03 - US-003
- Implemented authentication middleware in Medic/Core/auth_middleware.py
- Files changed: Medic/Core/auth_middleware.py (new), tests/unit/test_auth_middleware.py (new)
- **Learnings for future iterations:**
  - query_db() returns Optional[Union[str, List]] - use str() cast before json.loads when show_columns=True
  - Health endpoint bypass paths: /health, /v1/healthcheck, /metrics, /docs
  - Flask g context is used to store authenticated key info (api_key_id, api_key_name, api_key_scopes)
  - Admin scope grants all permissions (checked first in scope validation)
  - Pre-existing test failures in tests/unit/test_monitor.py due to missing slack_client module - not related to this work
---

## 2026-02-03 - US-004
- Added Prometheus metrics for authentication failures
- Files changed: Medic/Core/metrics.py, Medic/Core/auth_middleware.py
- **Learnings for future iterations:**
  - Prometheus metrics use Counter from prometheus_client for incrementing counters
  - Metrics are defined at module level (global) and imported where needed
  - Use labels for categorizing metrics (e.g., reason: invalid_key, expired_key, insufficient_scope)
  - record_*() helper functions are the pattern for incrementing metrics from other modules
  - Pre-existing flake8 errors in metrics.py (unused import, unused variable) - not related to this work
---

## 2026-02-03 - US-005
- Implemented rate limiting infrastructure in Medic/Core/rate_limiter.py
- Files changed: Medic/Core/rate_limiter.py (new), tests/unit/test_rate_limiter.py (new)
- **Learnings for future iterations:**
  - RateLimiter is an abstract base class (ABC) with InMemoryRateLimiter and RedisRateLimiter (placeholder) implementations
  - Sliding window algorithm tracks request timestamps per bucket (key:endpoint_type)
  - Default limits: 100 req/min heartbeats, 20 req/min management (configurable via RateLimitConfig)
  - Thread-safe with per-bucket locks (SlidingWindowEntry.lock) and global locks
  - Use check_rate_limit() convenience function for global limiter access
  - Per-key custom limits can be set via set_key_config() or set_key_rate_limit()
  - RateLimitResult contains: allowed, limit, remaining, reset_at, retry_after
  - Use get_rate_limiter() and set_rate_limiter() for global instance management
---

## 2026-02-03 - US-006
- Implemented rate limiting middleware in Medic/Core/rate_limit_middleware.py
- Files changed: Medic/Core/rate_limit_middleware.py (new), tests/unit/test_rate_limit_middleware.py (new)
- **Learnings for future iterations:**
  - @rate_limit() decorator should be applied AFTER @authenticate_request() so g.api_key_id is available
  - Auto-detects endpoint type based on path: /heartbeat/* paths use heartbeat limits, others use management limits
  - Falls back to IP-based rate limiting (ip:{remote_addr}) when no API key is present
  - Returns 429 with JSON body containing success: false, message, and retry_after
  - Headers always added: X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Reset
  - Retry-After header only added when rate limit is exceeded
  - Health endpoints bypass rate limiting (same as auth): /health, /v1/healthcheck, /metrics, /docs
  - verify_rate_limit() and get_rate_limit_headers() available for non-decorator usage
---

## 2026-02-03 - US-007
- Created webhooks database schema migration in migrations/002_create_webhooks.sql
- Files changed: migrations/002_create_webhooks.sql (new)
- **Learnings for future iterations:**
  - Foreign key references to services table use services(service_id) - not medic.services
  - JSONB is preferred for flexible JSON data (headers, payload)
  - Use CHECK constraints for enum-like values (status IN ('pending', 'success', 'failed', 'retrying'))
  - Partial indexes (WHERE clause) are useful for common filtered queries
  - ON DELETE CASCADE appropriate when child records should be deleted with parent
  - SQL migrations are not type-checked by mypy - only Python code is
---

## 2026-02-03 - US-008
- Implemented webhook delivery service in Medic/Core/webhook_delivery.py
- Files changed: Medic/Core/webhook_delivery.py (new), tests/unit/test_webhook_delivery.py (new)
- **Learnings for future iterations:**
  - WebhookDeliveryService sends POST requests with JSON payload and custom headers
  - Exponential backoff retry: 1s, 5s, 30s delays with max 3 attempts (configurable)
  - Delivery tracking via webhook_deliveries table with status: pending, success, failed, retrying
  - DeliveryStatus is a str Enum - can compare directly with strings (e.g., status == "pending")
  - Use deliver_webhook() convenience function for global service access
  - get_webhooks_for_service(service_id) returns webhooks for a service OR global webhooks
  - Thread-safe parallel delivery via deliver_to_all() with async_delivery=True
  - Response body truncated to 4096 bytes to prevent database bloat
  - Flake8 line length limit is 79 chars - break long lines appropriately
---

## 2026-02-03 - US-009
- Created teams database schema migration in migrations/003_create_teams.sql
- Files changed: migrations/003_create_teams.sql (new)
- **Learnings for future iterations:**
  - Teams table: team_id, name (unique), slack_channel_id (optional), timestamps
  - Services.team_id FK uses ON DELETE SET NULL (not CASCADE) - keeps service but clears team reference
  - Pattern: ALTER TABLE to add columns to existing tables (services already exists)
  - Partial index on slack_channel_id WHERE NOT NULL for efficient lookups
  - Migration pattern follows existing 001/002 - use medic.tablename for new tables, no schema prefix for services
---

## 2026-02-03 - US-010
- Implemented team-based alert routing in Medic/Core/alert_routing.py
- Files changed: Medic/Core/alert_routing.py (new), tests/unit/test_alert_routing.py (new)
- **Learnings for future iterations:**
  - get_slack_channel_for_service(service_id) returns team's Slack channel or falls back to SLACK_CHANNEL_ID env var
  - get_team_for_service(service_id) returns team dict with team_id, name, slack_channel_id or None
  - Query joins medic.teams and services tables via team_id FK
  - Empty string channel ("") is treated same as None - triggers fallback to default
  - Pattern: routing priority is team channel > default channel
---

## 2026-02-03 - US-011 - Linear Issue: SRE-10
- Created notification_targets database schema migration in migrations/004_create_notification_targets.sql
- Files changed: migrations/004_create_notification_targets.sql (new)
- **Learnings for future iterations:**
  - notification_targets table: target_id, service_id, type (slack/pagerduty/webhook), config (jsonb), priority, enabled, timestamps
  - Type constraint via CHECK ensures only valid types: slack, pagerduty, webhook
  - Priority field (integer) used for ordering targets in notify_until_success mode - lower = higher priority
  - Composite index on (service_id, priority) WHERE enabled = TRUE for common query pattern
  - Config field stores type-specific JSON (channel_id for slack, service_key for pagerduty, url/headers for webhook)
  - ON DELETE CASCADE from services - when service deleted, its targets are removed
---

## 2026-02-03 - US-012 - Linear Issue: SRE-10
- Implemented flexible alert routing logic in Medic/Core/alert_routing.py
- Files changed: Medic/Core/alert_routing.py (extended), tests/unit/test_alert_routing.py (extended)
- **Learnings for future iterations:**
  - NotificationMode enum: NOTIFY_ALL (send to all targets), NOTIFY_UNTIL_SUCCESS (stop after first success)
  - NotificationType enum: SLACK, PAGERDUTY, WEBHOOK (matches notification_targets.type column)
  - NotificationTarget dataclass: target_id, service_id, target_type, config, priority, enabled
  - NotificationResult dataclass: target_id, target_type, success, error_message
  - route_alert(service_id, payload, mode, sender) dispatches to all targets based on mode
  - get_notification_targets_for_service() retrieves targets ordered by priority (lower first)
  - Custom sender callback signature: (NotificationTarget, Dict[str, Any]) -> bool
  - default_notification_sender() dispatches by type to placeholder _send_*_notification() functions
  - Helper functions: has_notification_targets(), get_successful_results(), get_failed_results(), all_notifications_succeeded(), any_notification_succeeded()
  - Disabled targets are skipped but still return a NotificationResult with success=False
---

## 2026-02-03 - US-013 - Linear Issue: SRE-12
- Created schedules database schema migration in migrations/005_create_schedules.sql
- Files changed: migrations/005_create_schedules.sql (new)
- **Learnings for future iterations:**
  - schedules table: schedule_id, name (unique), timezone, hours (JSONB), timestamps
  - IANA timezone format enforced via CHECK constraint (Area/Location pattern, UTC, or Etc/ prefix)
  - Hours field uses JSONB for flexible day/hour ranges: {"monday": [{"start": "09:00", "end": "17:00"}], ...}
  - Services.schedule_id FK uses ON DELETE SET NULL (not CASCADE) - keeps service but clears schedule reference
  - Pattern follows teams migration: ALTER TABLE to add FK column to services
---

## 2026-02-03 - US-014 - Linear Issue: SRE-12
- Implemented working hours evaluation in Medic/Core/working_hours.py
- Files changed: Medic/Core/working_hours.py (new), tests/unit/test_working_hours.py (new)
- **Learnings for future iterations:**
  - Use zoneinfo.ZoneInfo for IANA timezone handling (Python 3.9+ standard library)
  - zoneinfo automatically handles DST transitions - just convert to local time and check
  - TimeRange.contains() handles midnight-crossing ranges (e.g., 22:00 to 06:00 night shifts)
  - Hours JSONB format: {"monday": [{"start": "09:00", "end": "17:00"}], ...}
  - End time is exclusive (09:00-17:00 means 17:00 is outside hours)
  - Empty timezone string causes ValueError in ZoneInfo - check with is_valid_timezone() first
  - is_service_within_working_hours() returns (True, None) when no schedule - services without schedules are always "within hours"
  - get_current_period() returns "during_hours" or "after_hours" for routing decisions
  - Naive datetimes are assumed to be UTC
---

## 2026-02-03 - US-015 - Linear Issue: SRE-12
- Implemented working hours-aware alert routing in Medic/Core/alert_routing.py
- Files changed: Medic/Core/alert_routing.py (extended), tests/unit/test_alert_routing.py (extended), migrations/006_add_period_to_notification_targets.sql (new)
- **Learnings for future iterations:**
  - NotificationPeriod enum: ALWAYS (default), DURING_HOURS, AFTER_HOURS - matches database period column values
  - Targets with period='always' are included in ALL routing decisions regardless of working hours
  - get_notification_targets_for_service() now accepts optional period parameter to filter by working hours period
  - SQL uses COALESCE(period, 'always') to handle NULL values from pre-migration data
  - route_alert_with_schedule() uses local import of get_service_current_period to avoid circular imports
  - When patching local imports in tests, patch at the source module (Medic.Core.working_hours.get_service_current_period), not the consuming module
  - Migration adds period column with DEFAULT 'always' so existing targets continue to work unchanged
---

## 2026-02-03 - US-016 - Linear Issue: SRE-14
- Created maintenance_windows database schema migration in migrations/007_create_maintenance_windows.sql
- Files changed: migrations/007_create_maintenance_windows.sql (new)
- **Learnings for future iterations:**
  - maintenance_windows table: window_id, name (unique), start_time, end_time, recurrence (nullable cron), timezone (IANA), service_ids (integer array), timestamps
  - Service_ids uses INTEGER[] array type with DEFAULT '{}' - empty array means "all services"
  - GIN index on service_ids enables efficient array containment queries (e.g., WHERE service_id = ANY(service_ids))
  - Recurrence field stores cron expression for recurring windows (NULL for one-time)
  - end_time > start_time enforced via CHECK constraint
  - IANA timezone constraint reused from schedules table pattern
  - Pattern follows earlier migrations: medic.tablename, SERIAL for ID, TIMESTAMP WITH TIME ZONE for times
---

## 2026-02-03 - US-017 - Linear Issue: SRE-14
- Implemented maintenance window evaluation in Medic/Core/maintenance_windows.py
- Files changed: Medic/Core/maintenance_windows.py (new), tests/unit/test_maintenance_windows.py (new), Medic/requirements.txt (added croniter)
- **Learnings for future iterations:**
  - MaintenanceWindow dataclass: window_id, name, start_time, end_time, timezone, recurrence, service_ids
  - is_service_in_maintenance(service_id, check_time) is the primary function for alert suppression logic
  - One-time windows: direct comparison of start_time <= check_time < end_time
  - Recurring windows: use croniter to find prev occurrence, add duration to get window end
  - croniter is optional - module works without it but recurring windows won't evaluate
  - Type ignore comments needed for croniter import: # type: ignore[import-untyped]
  - get_maintenance_status() returns detailed dict with window info and maintenance_end time
  - Empty service_ids array means window applies to ALL services - query with WHERE service_ids = '{}' OR %s = ANY(service_ids)
  - Feb 1, 2026 is a Sunday (weekday=6), Feb 2, 2026 is Monday (weekday=0) - always verify day calculations
  - parse_maintenance_window() handles both string and datetime objects from database
---

## 2026-02-03 - US-018 - Linear Issue: SRE-14
- Implemented maintenance window alert suppression in Medic/Worker/monitor.py
- Files changed: Medic/Worker/monitor.py (modified), tests/unit/test_monitor.py (extended)
- **Learnings for future iterations:**
  - Monitor module runs in Medic/Worker directory - needs PYTHONPATH=$(pwd)/Medic/Worker for tests
  - MAINTENANCE_WINDOWS_AVAILABLE flag allows graceful degradation if maintenance module not importable
  - Maintenance check happens BEFORE sendAlert() - only new alerts are suppressed
  - Recovery alerts (closeAlert) are NOT suppressed during maintenance - service can still recover
  - Logger level 20 = INFO for maintenance suppression messages
  - Pre-existing test failures in test_monitor.py due to slack_client module import - fixed with PYTHONPATH
  - When patching module-level variables like MAINTENANCE_WINDOWS_AVAILABLE, use @patch decorator on the module path
---

## 2026-02-03 - US-019 - Linear Issue: SRE-15
- Implemented start/complete signal statuses with HeartbeatStatus enum and run_id support
- Files changed: Medic/Helpers/heartbeat.py (modified), tests/unit/test_heartbeat.py (extended), migrations/008_add_heartbeat_status_and_run_id.sql (new)
- **Learnings for future iterations:**
  - HeartbeatStatus is a str Enum (inherits from both str and Enum) - can compare directly with strings
  - Use is_valid() class method to validate status strings, is_job_status() to check if job-related
  - run_id is optional on Heartbeat class - defaults to None for backward compatibility
  - heartbeatEvents table stores status as TEXT - enforcement is application-level not database-level
  - Use partial indexes (WHERE run_id IS NOT NULL) for run_id column since most heartbeats won't have it
  - Query functions updated to include run_id in SELECT - enables job correlation in results
  - Pre-existing mypy type errors in heartbeat.py - query_db returns Union[str, List, None] but functions declare Optional[str]
---

## 2026-02-03 - US-020 - Linear Issue: SRE-15
- Implemented V2 API endpoints for start/complete/fail signals
- Files changed: Medic/Core/routes.py (modified), tests/integration/test_api.py (extended)
- **Learnings for future iterations:**
  - V2 endpoints use /v2/heartbeat/<int:service_id>/start|complete|fail pattern with service_id in URL path
  - Shared logic extracted to _record_job_signal() helper function within exposeRoutes()
  - run_id is optional in JSON body - gracefully handles missing or invalid JSON with run_id=None
  - Response includes results dict with service_id, heartbeat_name, status, and run_id for caller verification
  - Integration tests use patch("Medic.Core.routes.db.query_db") and patch("Medic.Core.routes.hbeat.addHeartbeat") for mocking
  - Service validation checks: 1) service exists, 2) service is active (active != 0)
  - Returns 404 for missing service, 400 for inactive service, 500 for database insert failure
---

## 2026-02-03 - US-021 - Linear Issue: SRE-16
- Implemented duration tracking for job runs with medic.job_runs table and max_duration_ms column
- Files changed: migrations/009_create_job_runs_and_max_duration.sql (new), Medic/Core/job_runs.py (new), Medic/Core/routes.py (modified), tests/unit/test_job_runs.py (new), tests/integration/test_api.py (modified)
- **Learnings for future iterations:**
  - JobRun dataclass: run_id_pk, service_id, run_id, started_at, completed_at, duration_ms, status
  - record_job_start() creates new job_runs entry with STARTED status when run_id provided
  - record_job_completion() calculates duration_ms from started_at to completed_at (in milliseconds)
  - If no STARTED run exists when completing, creates a completion-only record with duration=0
  - Duration calculated as int(delta.total_seconds() * 1000) for millisecond precision
  - _record_job_signal() in routes.py integrates job_runs tracking after heartbeat is recorded
  - Integration tests need to mock job_runs module when testing V2 endpoints: patch("Medic.Core.routes.job_runs")
  - services.max_duration_ms column added for threshold-based alerting (to be implemented in US-023)
  - Unique constraint on (service_id, run_id) prevents duplicate runs per service
  - get_stale_runs() useful for finding jobs that started but never completed (hung jobs)
---

## 2026-02-03 - US-022 - Linear Issue: SRE-16
- Implemented duration statistics API endpoint in Medic/Core/routes.py
- Files changed: Medic/Core/job_runs.py (extended), Medic/Core/routes.py (extended), tests/unit/test_job_runs.py (extended), tests/integration/test_api.py (extended)
- **Learnings for future iterations:**
  - DurationStatistics dataclass provides structured response for stats endpoint
  - get_duration_statistics() calculates avg, p50, p95, p99 from completed runs with valid duration_ms
  - Minimum 5 runs required for meaningful statistics (returns nulls otherwise)
  - _percentile() uses linear interpolation (same as numpy default)
  - Percentile calculation: k = (n-1) * (p/100), then interpolate between floor and ceiling indices
  - GET /v2/services/:id/stats verifies service exists before calculating stats
  - Response includes min_duration_ms and max_duration_ms in addition to percentiles
  - Filter out null and negative duration values before calculating statistics
---

## 2026-02-03 - US-023 - Linear Issue: SRE-16
- Implemented duration threshold alerting for completed jobs and stale job detection
- Files changed: Medic/Core/job_runs.py (extended), Medic/Core/metrics.py (extended), Medic/Core/routes.py (extended), Medic/Worker/monitor.py (extended), tests/unit/test_job_runs.py (extended)
- **Learnings for future iterations:**
  - DurationAlert dataclass holds alert info: service_id, service_name, run_id, alert_type (exceeded/stale), duration_ms, max_duration_ms
  - check_duration_threshold(job_run) checks if completed job exceeded max_duration_ms, returns DurationAlert or None
  - get_stale_runs_exceeding_max_duration() joins job_runs with services to find STARTED jobs where elapsed time > max_duration_ms
  - mark_stale_run_alerted() sets status to STALE_ALERTED to prevent duplicate alerts
  - Duration check integrated in routes.py _record_job_signal() after job_runs.record_job_completion() is called
  - Stale job check added to monitor.py thread_function() - runs on same 15-second cycle as heartbeat monitoring
  - Import guards (DURATION_ALERTS_AVAILABLE) allow graceful degradation if module imports fail
  - Prometheus metrics: medic_duration_alerts_total (Counter with alert_type label), medic_stale_jobs_current (Gauge)
  - Stale job alerts include human-readable elapsed time (hours/minutes/seconds) in Slack message
---

## 2026-02-03 - US-024 - Linear Issue: SRE-17
- Implemented grace period support for heartbeat alert delays
- Files changed: migrations/010_add_grace_period_to_services.sql (new), Medic/Worker/monitor.py (modified), tests/unit/test_monitor.py (extended)
- **Learnings for future iterations:**
  - grace_period_seconds column added to services table with DEFAULT 0 (no grace period)
  - Grace period is checked before maintenance window check in queryForNoHeartbeat()
  - Grace period calculation: required_delay = (interval_minutes * 60) + grace_period_seconds
  - Alert delayed if time_since_last_heartbeat < required_delay
  - Use `heartbeat.get('grace_period_seconds', 0) or 0` to handle None values from legacy data
  - Naive datetimes are handled by localizing to UTC: `pytz.UTC.localize(last_hbeat_time)`
  - Grace period does not affect recovery alerts - only initial alerts are delayed
  - Use `continue` to skip to next service when within grace period
---

## 2026-02-03 - US-025 - Linear Issue: SRE-18
- Created playbooks and playbook_triggers database schema migration
- Files changed: migrations/011_create_playbooks.sql (new)
- **Learnings for future iterations:**
  - playbooks table: playbook_id, name (unique), description, yaml_content, version (default 1), timestamps
  - playbook_triggers table: trigger_id, playbook_id (FK with CASCADE delete), service_pattern (glob), consecutive_failures (default 1, must be > 0), enabled (boolean, default TRUE), timestamps
  - Pattern follows earlier migrations: medic.tablename, SERIAL for ID, TIMESTAMP WITH TIME ZONE for times
  - Service pattern uses glob syntax (e.g., "worker-*", "api-prod-*", "*") for flexible matching
  - Version field on playbooks enables audit trail - increment on each update
  - Composite index on (enabled, service_pattern, consecutive_failures) optimizes trigger matching queries
  - Partial index on enabled WHERE TRUE for efficient filtering of active triggers
---
