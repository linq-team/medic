## Codebase Patterns
- Script env security: Only pass allowlisted vars (PATH, HOME, USER, LANG, LC_ALL, TZ) via _get_script_env()
- Use `Medic.Helpers.logSettings.logSetup()` for logging configuration
- Follow existing module docstring pattern with description, features, and usage example
- Use `@dataclass` for configuration and status objects
- Test pattern: class-based tests with `@patch` decorators for mocking
- Use `pytest.mark.parametrize` for testing multiple inputs
- Import database module as `import Medic.Core.database as db`
- Module constants should be type-annotated: `CONSTANT: Type = value`
- Rate limiting: All endpoints must be rate limited - no bypass prefixes allowed
- Use `_get_endpoint_rate_limit_config()` to get endpoint-specific rate limits
- Environment variable config: Use `int(os.environ.get("VAR_NAME", "default"))` pattern
- **Datetime utilities: Import `from Medic.Core.utils.datetime_helpers import now as get_now, parse_datetime`**
- **When local var shadows import name, alias the import (e.g., `now as get_now`) to avoid `now = now()` conflicts**
- **Playbook package: Import models and db ops from `Medic.Core.playbook` (not playbook_engine)**
- **Test patches: Use `Medic.Core.playbook.db.db` for db operations moved to playbook package, `Medic.Core.playbook_engine.db` for functions remaining in engine**
- **Executor patches: When testing executor functions directly, patch at `Medic.Core.playbook.executors.<executor_name>.<function>` (e.g., `Medic.Core.playbook.executors.webhook.create_step_result`)**
- **Redis rate limiter: Use `REDIS_URL` env var for connection, `REDIS_POOL_SIZE` for pool (default 10), use `is_healthy()` to check connectivity**
- **Redis testing: Use `fakeredis` package for testing Redis-dependent code**
- **Rate limiter factory: Use `MEDIC_RATE_LIMITER_TYPE` env var (redis/memory/auto) for selection, `get_rate_limiter()` returns cached singleton**
- **Dockerfile: Multi-stage build (builder -> runtime), python:3.11-slim-bookworm base, port 8080, gunicorn WSGI server**
- **Docker user: Non-root user `medic` with uid 1000 for Kubernetes compatibility**
- **Docker Compose: Use `condition: service_healthy` for depends_on with healthchecks**
- **Helm chart: Use `helm lint` and `helm template` to validate charts before committing**
- **Helm helpers: Define reusable helpers in `_helpers.tpl` (medic.fullname, medic.labels, etc.)**
- **Helm values: Structure values.yaml with logical groupings (api, worker, ingress, config, secret)**
- **API Deployment: Use checksum annotations on configmap/secret for automatic pod restart on config changes**
- **Volume mounts: Add emptyDir for /tmp when using readOnlyRootFilesystem**
- **Worker deployment: Use exec-based liveness probe (not HTTP) for non-service deployments**
- **Service template: Use targetPort: http to reference named port from deployment**
- **Ingress template: Use conditional TLS annotations (certificate-arn, listen-ports, ssl-redirect) when TLS enabled**
- **Ingress TLS: Support both AWS ACM (certificateArn) and Kubernetes TLS secrets (secretName)**

- **ExternalSecret template: Use `external-secrets.io/v1beta1` API, target same secret name as direct secret**
- **ServiceAccount IRSA: Annotations go under `serviceAccount.annotations` with `eks.amazonaws.com/role-arn` key**
- **KEDA ScaledObject: Use `keda.sh/v1alpha1` API, reference deployment by name in scaleTargetRef**
- **KEDA HPA behavior: Scale up fast (stabilizationWindowSeconds: 15), scale down slow (stabilizationWindowSeconds: 300)**
- **KEDA CPU trigger: Use `type: cpu` with `metricType: Utilization` for percentage-based scaling**
- **PodDisruptionBudget: Use `policy/v1` API, selector must match deployment's selector labels**
- **Karpenter annotations: Add `karpenter.sh/do-not-disrupt: "false"` to allow node consolidation**
- **OpenTelemetry: Use `opentelemetry.exporter.otlp.proto.grpc.trace_exporter` for OTLP gRPC exporter**
- **Telemetry init: Call `init_telemetry(app)` after Flask app creation but before route registration**
- **Trace context: Use `get_current_trace_id()` from Flask g context for log correlation**

---

## 2026-02-04 - US-019 - Linear Issue: SRE-46
- **What was implemented**: OpenTelemetry instrumentation for distributed tracing
  - Added OpenTelemetry packages to `Medic/requirements.txt`:
    - `opentelemetry-api>=1.24.0`
    - `opentelemetry-sdk>=1.24.0`
    - `opentelemetry-instrumentation-flask>=0.45b0`
    - `opentelemetry-exporter-otlp-proto-grpc>=1.24.0`
  - Created `Medic/Core/telemetry.py` module with:
    - `get_otel_config()`: Read configuration from environment variables
    - `create_resource()`: Create OTEL Resource with service metadata
    - `create_tracer_provider()`: Configure TracerProvider with OTLP exporter
    - `setup_propagators()`: Configure W3C trace context propagation
    - `store_trace_context()`: Store trace_id/span_id in Flask g context
    - `get_current_trace_id()`: Get trace_id from Flask g for log correlation
    - `get_current_span_id()`: Get span_id from Flask g
    - `init_telemetry()`: Initialize OTEL for Flask application
    - `shutdown_telemetry()`: Gracefully shutdown telemetry
    - `get_tracer()`: Get tracer instance for custom spans
  - Environment variables supported:
    - `OTEL_EXPORTER_OTLP_ENDPOINT` (default: http://alloy:4317)
    - `OTEL_SERVICE_NAME` (default: medic)
    - `OTEL_RESOURCE_ATTRIBUTES` (comma-separated key=value pairs)
    - `MEDIC_ENVIRONMENT` (default: development)
    - `MEDIC_VERSION` (default: unknown)
    - `OTEL_ENABLED` (default: true)
  - Integrated telemetry into `medic.py` create_app()
  - Flask auto-instrumentation for request tracing
  - W3C trace context header propagation enabled
  - Added 28 unit tests covering all telemetry functions
  - Typecheck (mypy) and lint (flake8) pass

- **Files changed**:
  - `Medic/requirements.txt` (added OTEL packages)
  - `Medic/Core/telemetry.py` (new)
  - `medic.py` (added telemetry initialization)
  - `tests/unit/test_telemetry.py` (new, 28 tests)
  - `tasks/prd.json` (updated passes: true)

- **Learnings for future iterations**:
  - OpenTelemetry exporter import path is `opentelemetry.exporter.otlp.proto.grpc.trace_exporter` (not `proto_grpc`)
  - Use `insecure=True` for OTLP exporter when communicating within cluster
  - Trace IDs are 128-bit, format with `format(trace_id, "032x")` for 32-char hex string
  - Span IDs are 64-bit, format with `format(span_id, "016x")` for 16-char hex string
  - Initialize telemetry BEFORE registering routes so Flask instrumentation covers all endpoints
  - Use `before_request` hook to store trace context in Flask g for log correlation
  - Module-level `_initialized` flag prevents double initialization

---

## 2026-02-04 - US-018 - Linear Issue: SRE-45
- **What was implemented**: KEDA ScaledObject and PodDisruptionBudget Helm templates for autoscaling and availability
  - Created `helm/medic/templates/api-scaledobject.yaml` for KEDA autoscaling:
    - Uses `keda.sh/v1alpha1` API version
    - Conditional creation via `.Values.api.autoscaling.enabled`
    - Targets medic-api deployment via scaleTargetRef
    - Configurable min/max replicas, cooldownPeriod (300s), pollingInterval (30s)
    - CPU trigger with configurable threshold (default: 70%)
    - Optional Prometheus trigger for custom metrics (e.g., request rate)
    - HPA behavior: scale up fast (15s stabilization), scale down slow (300s)
    - Karpenter-friendly annotation `karpenter.sh/do-not-disrupt: "false"`
  - Created `helm/medic/templates/worker-pdb.yaml` for PodDisruptionBudget:
    - Uses `policy/v1` API version
    - Conditional creation via `.Values.worker.pdb.enabled`
    - Configurable minAvailable (default: 1)
    - Selector matches worker deployment labels
    - Karpenter-friendly annotation for node consolidation
  - Added topology spread constraints to worker-deployment.yaml:
    - Previously only API deployment had topologySpreadConstraints
    - Now worker also distributes across availability zones
  - Verified `helm lint` passes
  - Verified `helm template` renders valid YAML
  - Verified `kubectl apply --dry-run=client` succeeds

- **Files changed**:
  - `helm/medic/templates/api-scaledobject.yaml` (new)
  - `helm/medic/templates/worker-pdb.yaml` (new)
  - `helm/medic/templates/worker-deployment.yaml` (added topologySpreadConstraints)
  - `tasks/prd.json` (updated passes: true)

- **Learnings for future iterations**:
  - KEDA ScaledObject replaces HPA - use `keda.sh/v1alpha1` API
  - ScaledObject's scaleTargetRef needs apiVersion, kind, and name fields
  - CPU trigger uses `metricType: Utilization` for percentage-based scaling
  - Prometheus trigger requires serverAddress, metricName, threshold, and query
  - HPA behavior block in `advanced.horizontalPodAutoscalerConfig` controls scaling speed
  - PodDisruptionBudget uses `policy/v1` (not `policy/v1beta1`)
  - PDB selector must exactly match deployment's selector labels
  - Karpenter annotation `karpenter.sh/do-not-disrupt: "false"` enables node consolidation
  - When autoscaling enabled, omit `replicas` from Deployment to avoid KEDA conflicts

---

## 2026-02-04 - US-017 - Linear Issue: SRE-44
- **What was implemented**: ConfigMap, Secret, and ServiceAccount Helm templates for configuration management
  - ConfigMap (`configmap.yaml`) was already created in US-014:
    - Iterates over `.Values.config` map to create key-value pairs
    - Used for non-sensitive configuration (PORT, LOG_LEVEL, rate limits)
  - Secret (`secret.yaml`) was already created in US-014:
    - Conditional creation via `.Values.secret.create`
    - Values base64-encoded automatically via `b64enc`
    - Used for sensitive values when not using ESO
  - Created `helm/medic/templates/external-secret.yaml` for ESO integration:
    - Uses `external-secrets.io/v1beta1` API version
    - Controlled by `.Values.externalSecret.enabled`
    - References ClusterSecretStore configured in values
    - Creates secret with same name as direct secret (`medic-secret`)
    - Supports configurable refresh interval (default: 1h)
    - Deletion policy set to Retain for safety
  - Created `helm/medic/templates/serviceaccount.yaml`:
    - Conditional creation via `.Values.serviceAccount.create`
    - Supports custom annotations for IRSA (`eks.amazonaws.com/role-arn`)
    - automountServiceAccountToken enabled
    - Referenced by both API and Worker deployments
  - Values.yaml already had all required defaults from US-013
  - Verified `helm lint` passes
  - Verified `helm template` renders correctly for all combinations
  - Verified `kubectl apply --dry-run=client` validates all resources

- **Files changed**:
  - `helm/medic/templates/external-secret.yaml` (new)
  - `helm/medic/templates/serviceaccount.yaml` (new)
  - `tasks/prd.json` (updated passes: true)

- **Learnings for future iterations**:
  - ExternalSecret `target.name` should match the name used by direct secrets for consistent envFrom references
  - Use `deletionPolicy: Retain` for ExternalSecrets to prevent accidental secret deletion
  - ServiceAccount annotations are the standard way to enable IRSA on EKS
  - Test all value combinations (ESO on/off, secret.create on/off, custom SA name) with `helm template`

---

## 2026-02-04 - US-016 - Linear Issue: SRE-43
- **What was implemented**: Service and Ingress Helm templates for external access
  - Created `helm/medic/templates/api-service.yaml` - ClusterIP service:
    - Service name: `{{ include "medic.fullname" . }}-api`
    - Service type: `{{ .Values.service.type }}` (default: ClusterIP)
    - Port configurable via `{{ .Values.service.port }}` (default: 8080)
    - Named port `http` for reference by ingress
    - Selector uses `medic.api.selectorLabels` helper
  - Created `helm/medic/templates/ingress.yaml` with conditional creation:
    - Controlled by `{{ .Values.ingress.enabled }}`
    - Uses `networking.k8s.io/v1` API version
    - Supports `ingressClassName` for ALB controller
    - ALB annotations configurable via `{{ .Values.ingress.annotations }}`
    - Host configurable via `{{ .Values.ingress.host }}`
    - TLS support with two options:
      - AWS ACM: `certificateArn` adds ALB certificate annotations
      - K8s secret: `secretName` for standard TLS secrets
    - Automatic SSL redirect when TLS enabled with ACM certificate
  - Updated `values.yaml` with TLS secretName field
  - Verified `helm lint` passes
  - Verified `helm template` renders valid YAML
  - Verified `kubectl apply --dry-run=client` succeeds

- **Files changed**:
  - `helm/medic/templates/api-service.yaml` (new)
  - `helm/medic/templates/ingress.yaml` (new)
  - `helm/medic/values.yaml` (added tls.secretName)
  - `tasks/prd.json` (updated passes: true)

- **Learnings for future iterations**:
  - Use `targetPort: http` (named port) instead of hardcoding port number for flexibility
  - Ingress annotations for ALB TLS include certificate-arn, listen-ports, and ssl-redirect
  - Support both ACM certificates (AWS-managed) and K8s TLS secrets (for other ingress controllers)
  - Conditional annotation blocks can be nested inside `{{- with }}` blocks
  - Ingress paths should use `pathType: Prefix` for catch-all routing

---

## 2026-02-04 - US-015 - Linear Issue: SRE-42
- **What was implemented**: Worker Deployment Helm template for Kubernetes deployment
  - Created `helm/medic/templates/worker-deployment.yaml` with full Deployment spec:
    - Deployment name: `{{ include "medic.fullname" . }}-worker`
    - Replicas: `{{ .Values.worker.replicaCount }}` (default: 1)
    - Container command: `['python', '-m', 'Medic.Worker.monitor']`
    - Resources from `{{ .Values.worker.resources }}`
    - Liveness probe: exec-based (python health check) with configurable delays
    - Security context matching API deployment (runAsNonRoot, readOnlyRootFilesystem)
    - Environment variables shared with API via envFrom
    - Checksum annotations for config/secret to trigger pod restart on changes
    - Volume mount for /tmp (emptyDir) to support readOnlyRootFilesystem
    - Shared affinity, tolerations, and nodeSelector with API
  - Worker values.yaml defaults already existed in values.yaml (lines 75-102)
  - Verified `helm lint` passes
  - Verified `helm template` renders valid deployment YAML
  - Verified with `kubectl apply --dry-run=client` for Kubernetes validation

- **Files changed**:
  - `helm/medic/templates/worker-deployment.yaml` (new)
  - `tasks/prd.json` (updated passes: true)

- **Learnings for future iterations**:
  - Worker uses exec-based liveness probe (not HTTP) since it doesn't expose ports
  - Worker does not need readinessProbe since it has no service/ingress
  - Worker does not have ports section since it's not a service
  - Worker shares same securityContext, envFrom, and volumes as API for consistency
  - No autoscaling support for worker (single replica is typical for monitor workers)

---

## 2026-02-04 - US-014 - Linear Issue: SRE-41
- **What was implemented**: API Deployment Helm template for Kubernetes deployment
  - Created `helm/medic/templates/api-deployment.yaml` with full Deployment spec:
    - Deployment name: `{{ include "medic.fullname" . }}-api`
    - Replicas from values with KEDA autoscaling support (omits replicas when autoscaling enabled)
    - Image from `{{ .Values.image.repository }}:{{ .Values.image.tag }}`
    - Resources from `{{ .Values.api.resources }}`
    - Configurable liveness probe: /health/live with initialDelaySeconds, periodSeconds, timeoutSeconds, failureThreshold
    - Configurable readiness probe: /health/ready with same configurable delays
    - Pod security context: runAsNonRoot, runAsUser 1000, fsGroup 1000
    - Container security context: allowPrivilegeEscalation false, readOnlyRootFilesystem true, drop ALL capabilities
    - Pod anti-affinity for spreading across nodes (preferredDuringSchedulingIgnoredDuringExecution)
    - Topology spread constraints for Karpenter multi-AZ distribution (topology.kubernetes.io/zone)
    - Environment variables via envFrom from ConfigMap and optional Secret
    - Checksum annotations for config/secret to trigger pod restart on changes
    - Volume mount for /tmp (emptyDir) to support readOnlyRootFilesystem
  - Created `helm/medic/templates/configmap.yaml` for non-sensitive configuration
  - Created `helm/medic/templates/secret.yaml` for sensitive values (conditional)
  - Verified `helm lint` passes
  - Verified `helm template` renders valid deployment YAML

- **Files changed**:
  - `helm/medic/templates/api-deployment.yaml` (new)
  - `helm/medic/templates/configmap.yaml` (new)
  - `helm/medic/templates/secret.yaml` (new)
  - `tasks/prd.json` (updated passes: true)

- **Learnings for future iterations**:
  - Use checksum annotations (`checksum/config`, `checksum/secret`) to trigger pod restarts on ConfigMap/Secret changes
  - When using readOnlyRootFilesystem, mount emptyDir at /tmp for apps that need temp file access
  - Omit `replicas` field when KEDA autoscaling is enabled to avoid conflicts
  - Use `include` instead of `template` for helpers that return values (include allows piping)
  - Pod template labels must match selector labels exactly for Deployment to work
  - topologySpreadConstraints with `whenUnsatisfiable: ScheduleAnyway` provides soft multi-AZ distribution

---

## 2026-02-04 - US-013 - Linear Issue: SRE-40
- **What was implemented**: Helm chart structure for Medic deployment to Kubernetes
  - Created `helm/medic/Chart.yaml` with chart metadata (name, version, appVersion)
  - Created `helm/medic/values.yaml` with comprehensive default values:
    - Image configuration (repository, tag, pullPolicy)
    - API deployment (replicaCount, resources, probes, KEDA autoscaling)
    - Worker deployment (replicaCount, resources, probes, PDB)
    - Service configuration (ClusterIP, port 8080)
    - Ingress configuration (ALB support, TLS, host)
    - ConfigMap for non-sensitive config (PORT, LOG_LEVEL, rate limiting)
    - Secret management (direct secrets or ExternalSecret/ESO)
    - ServiceAccount with IRSA annotations support
    - Security contexts (runAsNonRoot, readOnlyRootFilesystem)
    - Affinity and topology spread for multi-AZ distribution
    - ServiceMonitor for Prometheus/Alloy metrics
    - Grafana dashboard support
    - Database migrations hook support
  - Created `helm/medic/templates/_helpers.tpl` with reusable template helpers:
    - medic.name, medic.fullname, medic.chart
    - medic.labels, medic.selectorLabels
    - medic.api.selectorLabels, medic.worker.selectorLabels
    - medic.serviceAccountName, medic.namespace
    - medic.configmapName, medic.secretName
    - medic.image, medic.envFrom
    - medic.podSecurityContext, medic.securityContext
  - Created `helm/medic/templates/namespace.yaml` (conditional, controlled by value)
  - Created `helm/medic/templates/NOTES.txt` with post-install instructions
  - Verified `helm lint` passes (only INFO about optional icon)
  - Verified `helm template` renders valid YAML

- **Files changed**:
  - `helm/medic/Chart.yaml` (new)
  - `helm/medic/values.yaml` (new)
  - `helm/medic/templates/_helpers.tpl` (new)
  - `helm/medic/templates/namespace.yaml` (new)
  - `helm/medic/templates/NOTES.txt` (new)
  - `tasks/prd.json` (updated passes: true)

- **Learnings for future iterations**:
  - Use `helm lint` before committing to catch YAML syntax errors
  - Use `helm template` to verify rendered output without deploying
  - Organize values.yaml with clear sections and comments for maintainability
  - Template helpers (`_helpers.tpl`) reduce duplication across templates
  - Use conditional rendering ({{- if .Values.x }}) for optional resources
  - IRSA annotations go on ServiceAccount for AWS IAM integration
  - topologySpreadConstraints enable Karpenter-aware scheduling
  - External Secrets Operator (ESO) integration via externalSecret values

---

## 2026-02-04 - US-012 - Linear Issue: SRE-39
- **What was implemented**: Docker Compose for local development with all dependencies
  - Updated `docker-compose.yml` with production-ready service structure
  - Service: postgres (postgres:15) with persistent volume and healthcheck
  - Service: redis (redis:7) with persistent volume, healthcheck, and AOF persistence
  - Service: medic-api (builds from Dockerfile, exposes port 8080, healthcheck)
  - Service: medic-worker (builds from Dockerfile, runs Medic.Worker.monitor)
  - All app services depend on postgres and redis with `condition: service_healthy`
  - Environment variables use ${VAR:-default} syntax for flexibility
  - Updated `.env.example` with all required environment variables including:
    - Database config (DB_HOST, DB_PORT, DB_NAME, PG_USER, PG_PASS)
    - Application config (PORT, DEBUG, MEDIC_BASE_URL, MEDIC_TIMEZONE, LOG_LEVEL)
    - Worker settings (WORKER_INTERVAL_SECONDS, ALERT_AUTO_UNMUTE_HOURS)
    - Redis config (REDIS_URL, MEDIC_RATE_LIMITER_TYPE, REDIS_POOL_SIZE)
    - Slack/PagerDuty integration (optional)
    - Security config (MEDIC_SECRETS_KEY, MEDIC_WEBHOOK_SECRET, MEDIC_ALLOWED_WEBHOOK_HOSTS)
    - Rate limiting config (RATE_LIMIT_*_REQUESTS)
  - Verified `docker-compose config` validates successfully

- **Files changed**:
  - `docker-compose.yml` (complete rewrite with 4 services, healthchecks, volumes)
  - `.env.example` (updated with all environment variables)
  - `tasks/prd.json` (updated passes: true)

- **Learnings for future iterations**:
  - Use `condition: service_healthy` in depends_on for reliable startup order
  - Don't use `env_file: .env` if the file might not exist (causes validation failure)
  - Use ${VAR:-default} syntax for environment variables with sensible defaults
  - Redis `--appendonly yes` enables persistence via AOF (append-only file)
  - Service names (postgres, redis) become DNS names for inter-container communication
  - Worker command: `["python", "-m", "Medic.Worker.monitor"]` for module execution

---

## 2026-02-04 - US-011 - Linear Issue: SRE-38
- **What was implemented**: Production Dockerfile with multi-arch support (amd64 and arm64)
  - Multi-stage build: builder stage for installing dependencies, runtime stage for minimal app image
  - Base image: `python:3.11-slim-bookworm` (supports both amd64 and arm64 architectures)
  - Only production dependencies from requirements.txt installed (not dev dependencies)
  - Non-root user `medic` with uid 1000 created for Kubernetes security policies
  - Environment variables: `PYTHONUNBUFFERED=1`, `PYTHONDONTWRITEBYTECODE=1`
  - Exposed port 8080 (production standard)
  - HEALTHCHECK instruction: `curl -f http://localhost:8080/health || exit 1`
  - Labels: maintainer, version, description
  - Gunicorn WSGI server with 2 workers and 4 threads for production performance
  - Created `.dockerignore` excluding: .git, tests/, __pycache__, *.pyc, .env, .venv, tasks/, docs/

- **Files changed**:
  - `Dockerfile` (rewrote with multi-stage build and production optimizations)
  - `.dockerignore` (new file for Docker build context optimization)
  - `tasks/prd.json` (updated passes: true)

- **Learnings for future iterations**:
  - Use `python:3.11-slim-bookworm` for multi-arch support (not alpine - has musl issues)
  - Install `libpq5` in runtime stage (not `libpq-dev`) for psycopg2 runtime
  - Copy Python packages from builder with `COPY --from=builder /root/.local /home/medic/.local`
  - Set uid 1000 explicitly for Kubernetes SecurityContext compatibility
  - Use gunicorn instead of Flask dev server for production
  - All Python dependencies (flask, psycopg2-binary, cryptography, etc.) have pre-built wheels for both amd64 and arm64

---

## 2026-02-04 - US-010 - Linear Issue: SRE-37
- **What was implemented**: Rate limiter factory with automatic Redis/InMemory selection
  - Enhanced `get_rate_limiter()` to auto-select between Redis and InMemory based on config
  - Added `_create_rate_limiter()` internal factory with selection logic
  - Added `MEDIC_RATE_LIMITER_TYPE` env var support: 'redis', 'memory', 'auto' (default: 'auto')
  - Auto mode: Tries Redis if `REDIS_URL` set, falls back to InMemory on failure
  - Redis mode: Forces Redis (raises ValueError if `REDIS_URL` not set)
  - Memory mode: Forces InMemory (ignores Redis even if configured)
  - Logs INFO when selecting limiter, WARNING on Redis fallback
  - Type is case-insensitive (MEMORY, Memory, memory all work)
  - Unknown types fall back to InMemory with WARNING log
  - Limiter instance cached as module-level singleton
  - Middleware already uses `check_rate_limit()` which calls factory - no changes needed

- **Files changed**:
  - `Medic/Core/rate_limiter.py` (enhanced get_rate_limiter, added _create_rate_limiter)
  - `tests/unit/test_rate_limiter.py` (added 14 TestRateLimiterFactory tests)
  - `tasks/prd.json` (updated passes: true)

- **Learnings for future iterations**:
  - Use `_create_rate_limiter()` for the actual creation logic, `get_rate_limiter()` for singleton
  - Test Redis fallback by mocking `_create_redis_client()` and `ping()` methods
  - Mock at `Medic.Core.rate_limiter.RedisRateLimiter._create_redis_client` for client creation
  - Use `set_rate_limiter(None)` in test setup/teardown to reset singleton state
  - Type constants (RATE_LIMITER_TYPE_*) make code more maintainable than string literals

---

## 2026-02-04 - US-009 - Linear Issue: SRE-36
- **What was implemented**: Redis-backed distributed rate limiting for multi-replica deployments
  - Replaced `NotImplementedError` with full `RedisRateLimiter` implementation
  - Used Redis sorted sets (ZSET) for sliding window algorithm
  - Each request stored with timestamp as score for efficient window-based counting
  - Used Redis pipeline with MULTI/EXEC for atomic operations
  - Added `_create_redis_client()` for automatic client creation from `REDIS_URL`
  - Added `REDIS_POOL_SIZE` configuration (default: 10)
  - Added `is_healthy()` method to check Redis connectivity
  - Added key expiry (window_seconds + 1) for automatic cleanup
  - Added 20 unit tests using `fakeredis` for Redis mocking

- **Files changed**:
  - `Medic/Core/rate_limiter.py` (implemented RedisRateLimiter class)
  - `Medic/requirements.txt` (added redis>=5.0.0)
  - `requirements-dev.txt` (added fakeredis>=2.21.0, types-redis>=4.6.0)
  - `tests/unit/test_rate_limiter.py` (added 20 RedisRateLimiter tests)
  - `tasks/prd.json` (updated passes: true)

- **Learnings for future iterations**:
  - Redis sorted sets (ZSET) are ideal for sliding window rate limiting
  - Use `zadd()` with timestamp as score, `zremrangebyscore()` to remove old entries
  - Use `zcard()` to count current entries, `zrange()` with `withscores=True` for oldest entry
  - Always set key expiry with `expire()` for automatic cleanup
  - Use `redis.ConnectionPool.from_url()` for connection pooling
  - `fakeredis` package provides in-memory Redis for testing without real Redis server
  - Test `is_healthy()` by mocking `ping()` to raise exceptions

---

## 2026-02-04 - US-008 - Linear Issue: SRE-35
- **What was implemented**: Extracted playbook step executors to separate modules
  - Created `Medic/Core/playbook/executors/` package directory
  - Created `Medic/Core/playbook/executors/__init__.py` with exports for all executors
  - Created `Medic/Core/playbook/executors/webhook.py` with:
    - `execute_webhook_step()`: HTTP request execution with variable/secret substitution
    - `substitute_variables()`: ${VAR_NAME} syntax substitution
    - `substitute_all()`: Combined variable and secret substitution
    - `_build_webhook_context()`: Build execution context for substitutions
  - Created `Medic/Core/playbook/executors/script.py` with:
    - `execute_script_step()`: Pre-registered script execution with resource limits
    - `get_registered_script()`: Fetch script from database
    - `RegisteredScript`: Dataclass for script metadata
    - `_get_script_env()`: Build secure environment (allowlist-based)
    - `_substitute_script_variables()`: Script content substitution
  - Created `Medic/Core/playbook/executors/condition.py` with:
    - `execute_condition_step()`: Condition polling with timeout
    - `check_heartbeat_received()`: Heartbeat condition checker
  - Created `Medic/Core/playbook/executors/wait.py` with:
    - `execute_wait_step()`: Simple duration-based wait
  - Updated `playbook_engine.py` to import from executors and re-export for backwards compatibility
  - Updated 50+ test patches to use correct module paths

- **Files changed**:
  - `Medic/Core/playbook/executors/__init__.py` (new)
  - `Medic/Core/playbook/executors/webhook.py` (new)
  - `Medic/Core/playbook/executors/script.py` (new)
  - `Medic/Core/playbook/executors/condition.py` (new)
  - `Medic/Core/playbook/executors/wait.py` (new)
  - `Medic/Core/playbook_engine.py` (refactored imports, removed executor code)
  - `tests/unit/test_playbook_engine.py` (updated 50+ mock patches)
  - `tasks/prd.json` (updated passes: true)

- **Learnings for future iterations**:
  - When extracting functions to new modules, ALL patches must be updated to new locations
  - Patches must target where functions are IMPORTED/USED, not where they're defined
  - For executor functions, patch at `Medic.Core.playbook.executors.<executor>.<function>`
  - Re-exports with `# noqa: F401` on import line maintains backwards compatibility
  - Testing scripts that use db.query_db requires patching `Medic.Core.playbook.executors.<module>.db`
  - Executor tests that directly call the function need patches at the executor module level
  - Engine tests that use execute_step() may need patches at wait module for WaitStep tests

---

## 2026-02-04 - US-007 - Linear Issue: SRE-34
- **What was implemented**: Created playbook package structure for maintainability
  - Created `Medic/Core/playbook/` package directory
  - Created `Medic/Core/playbook/models.py` with dataclasses:
    - `ExecutionStatus`: Enum for playbook execution states
    - `StepResultStatus`: Enum for step result states
    - `StepResult`: Dataclass for step execution results
    - `PlaybookExecution`: Dataclass for playbook execution instances
    - `StepExecutor`: Type alias for step executor functions
  - Created `Medic/Core/playbook/db.py` with all database operations:
    - Execution: create_execution, get_execution, get_active_executions, etc.
    - Step results: create_step_result, update_step_result, etc.
    - Playbook loading: get_playbook_by_id
  - Created `Medic/Core/playbook/__init__.py` with public API exports
  - Updated `playbook_engine.py` to import from new package
  - Updated test patches to use correct module paths

- **Files changed**:
  - `Medic/Core/playbook/__init__.py` (new)
  - `Medic/Core/playbook/models.py` (new)
  - `Medic/Core/playbook/db.py` (new)
  - `Medic/Core/playbook_engine.py` (refactored imports, removed duplicated code)
  - `tests/unit/test_playbook_engine.py` (updated mock patches)
  - `tasks/prd.json` (updated passes: true)

- **Learnings for future iterations**:
  - When moving database functions to a new module, tests need patches updated to new location
  - Functions in playbook.db module need `@patch('Medic.Core.playbook.db.db')` for db mocking
  - Functions remaining in playbook_engine need `@patch('Medic.Core.playbook_engine.db')` for db mocking
  - TYPE_CHECKING imports help avoid circular imports for type hints
  - Package __init__.py can re-export all public API for convenience imports
  - Keep backwards compatibility by importing and re-exporting from old module location

---

## 2026-02-04 - US-006 - Linear Issue: SRE-33
- **What was implemented**: Extracted shared datetime utilities to single module
  - Created `Medic/Core/utils/__init__.py` (empty package init)
  - Created `Medic/Core/utils/datetime_helpers.py` with:
    - `TIMEZONE: pytz.BaseTzInfo = pytz.timezone('America/Chicago')` constant
    - `now() -> datetime` function for timezone-aware current time
    - `parse_datetime(dt_str: str) -> Optional[datetime]` for multi-format parsing
  - Replaced 6 duplicate `_now()` implementations with centralized `get_now()`
  - Replaced 4 duplicate `_parse_datetime()` implementations with centralized `parse_datetime()`
  - Removed unused `pytz` imports from modules using datetime_helpers

- **Files changed**:
  - `Medic/Core/utils/__init__.py` (new)
  - `Medic/Core/utils/datetime_helpers.py` (new)
  - `Medic/Core/playbook_engine.py` (import datetime_helpers, remove _now/_parse_datetime)
  - `Medic/Core/audit_log.py` (import datetime_helpers, remove _now/_parse_datetime)
  - `Medic/Core/slack_approval.py` (import datetime_helpers, remove _now/_parse_datetime)
  - `Medic/Core/secrets.py` (import datetime_helpers, remove _now)
  - `Medic/Core/circuit_breaker.py` (import datetime_helpers, remove _now)
  - `Medic/Core/job_runs.py` (import datetime_helpers, remove _parse_datetime)
  - `tests/unit/test_circuit_breaker.py` (update mocks for get_now)
  - `tests/unit/test_job_runs.py` (update imports for parse_datetime)
  - `tests/unit/test_playbook_engine.py` (update imports for parse_datetime)
  - `tasks/prd.json` (updated passes: true)

- **Learnings for future iterations**:
  - When importing a function that will be used as `var = func()`, alias the import to avoid shadowing
  - Example: `from module import now as get_now` prevents `now = now()` from failing
  - Mock paths must match the aliased import name (e.g., `@patch('module.get_now')` not `@patch('module.now')`)
  - pytz.BaseTzInfo is the correct type hint for timezone objects
  - Empty `__init__.py` creates a Python package; no content needed for basic package structure
  - Centralized utilities reduce code duplication but require updating all tests that mock the old functions

---

## 2026-02-04 - US-005 - Linear Issue: SRE-32
- **What was implemented**: Added rate limiting to ALL endpoints (no bypasses)
  - Removed `/metrics` and `/docs` from `RATE_LIMIT_BYPASS_PREFIXES`
  - Replaced bypass mechanism with endpoint-specific rate limits
  - Added `RATE_LIMIT_HEALTH_REQUESTS` config (default: 1000 req/min) for /health
  - Added `RATE_LIMIT_METRICS_REQUESTS` config (default: 100 req/min) for /metrics
  - Added `RATE_LIMIT_DOCS_REQUESTS` config (default: 60 req/min) for /docs
  - Created `_get_endpoint_rate_limit_config()` to return appropriate config
  - Updated `_determine_endpoint_type()` to detect health, metrics, docs types
  - Removed `_should_bypass_rate_limit()` function entirely
  - All 47 unit tests pass verifying no endpoints bypass rate limiting

- **Files changed**:
  - `Medic/Core/rate_limit_middleware.py` (refactored rate limit bypass to endpoint-specific limits)
  - `tests/unit/test_rate_limit_middleware.py` (updated and added 47 tests)
  - `tasks/prd.json` (updated passes: true)

- **Learnings for future iterations**:
  - Security best practice: Never bypass rate limiting - use endpoint-specific limits instead
  - When changing bypass behavior, integration tests need custom configs explicitly passed
  - Endpoint type constants (`ENDPOINT_TYPE_*`) help maintain consistency across functions
  - Rate limit middleware applies endpoint-specific config when no custom config provided
  - Tests for "no bypass" should verify the check function IS called (not assert_not_called)

---

## 2026-02-03 22:00 - US-001 - Linear Issue: SRE-28
- **What was implemented**: URL validator module for SSRF prevention
  - Created `Medic/Core/url_validator.py` with:
    - `InvalidURLError` exception class
    - `validate_url(url: str, skip_dns_check: bool = False) -> bool` function
    - `is_private_ip(ip: str) -> bool` helper function
    - `is_safe_url(url: str) -> bool` convenience wrapper
    - `get_allowed_hosts() -> Optional[Set[str]]` for env var support
    - `resolve_hostname(hostname: str) -> List[str]` for DNS rebinding prevention
  - Blocks private IP ranges: 127.0.0.0/8, 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16, 169.254.0.0/16, 0.0.0.0/8
  - Blocks IPv6 private ranges: ::1/128, fc00::/7, fe80::/10, ::/128
  - Blocks localhost, 0.0.0.0, cloud metadata (169.254.169.254)
  - Only allows http/https schemes
  - Performs DNS resolution to catch DNS rebinding attacks
  - Supports `MEDIC_ALLOWED_WEBHOOK_HOSTS` env var for explicit allowlist
  - Error messages are generic to prevent information leakage

- **Files changed**:
  - `Medic/Core/url_validator.py` (new)
  - `tests/unit/test_url_validator.py` (new, 128 tests)
  - `tasks/prd.json` (updated passes: true)

- **Learnings for future iterations**:
  - The codebase uses `logger.log(level=30, msg="...")` pattern instead of `logger.warning()`
  - Type annotations are important - mypy is strict about List types needing explicit annotation
  - IPv6 networks need separate handling from IPv4 networks in type annotations
  - Tests should use `skip_dns_check=True` when testing URL validation without network calls
  - `socket.getaddrinfo` returns tuples where index [4][0] is the IP address
  - Use `patch.dict('os.environ', {...})` for testing environment variables

---

## 2026-02-04 00:30 - US-003 - Linear Issue: SRE-30
- **What was implemented**: Fixed timing attack vulnerability in API key verification
  - Modified `_get_api_key_from_db()` in `auth_middleware.py` to prevent timing attacks
  - Instead of returning immediately when a match is found, the function now:
    - Stores matched key in a variable
    - Continues iterating through ALL keys
    - Returns matched key only after complete iteration
  - Added detailed code comment explaining the timing attack mitigation
  - Added 4 unit tests verifying all keys are checked regardless of match position

- **Files changed**:
  - `Medic/Core/auth_middleware.py` (modified _get_api_key_from_db function)
  - `tests/unit/test_auth_middleware.py` (new TestGetApiKeyFromDbTimingAttack class with 4 tests)
  - `tasks/prd.json` (updated passes: true)

- **Learnings for future iterations**:
  - Timing attacks can be used to enumerate API keys by measuring response times
  - Always iterate through all items in security-sensitive comparisons to maintain constant time
  - Use `mock.side_effect = [True, False, False, ...]` to simulate different match positions in tests
  - Verify timing attack mitigation by asserting `mock.call_count == total_items`
  - Pre-existing lint errors (E501 line-too-long) are in the codebase - flake8 config may need updating

---

## 2026-02-03 23:15 - US-002 - Linear Issue: SRE-29
- **What was implemented**: Integrated URL validator into webhook execution
  - Added `from Medic.Core.url_validator import InvalidURLError, validate_url` to `playbook_engine.py`
  - Call `validate_url(url)` in `execute_webhook_step()` after variable substitution, before HTTP request
  - On `InvalidURLError`, return `StepResult` with `FAILED` status and "Invalid webhook URL" message
  - Added `from Medic.Core.url_validator import InvalidURLError, validate_url` to `webhook_delivery.py`
  - Call `validate_url(url)` in `_send_request()` before HTTP request
  - Log validation failures at WARNING level using `logger.warning()`
  - Added 11 integration tests (5 in test_playbook_engine.py, 6 in test_webhook_delivery.py)

- **Files changed**:
  - `Medic/Core/playbook_engine.py` (import + validation in execute_webhook_step)
  - `Medic/Core/webhook_delivery.py` (import + validation in _send_request)
  - `tests/unit/test_playbook_engine.py` (new TestExecuteWebhookStepSSRFPrevention class)
  - `tests/unit/test_webhook_delivery.py` (new TestWebhookDeliveryServiceSSRFPrevention class)
  - `tasks/prd.json` (updated passes: true)

- **Learnings for future iterations**:
  - URL validation should happen AFTER variable substitution so ${secrets.XXX} and ${VAR} are resolved first
  - Use `@patch('Medic.Core.module.validate_url')` to mock URL validation in tests
  - The playbook_engine uses `StepResult` dataclass for return values with specific status codes
  - webhook_delivery uses `DeliveryResult` dataclass with `success` and `error_message` fields
  - When validation fails, HTTP client should NOT be called - verify with `mock_client.assert_not_called()`

---

## 2026-02-04 01:45 - US-004 - Linear Issue: SRE-31
- **What was implemented**: Fixed script execution environment variable leak
  - Created `ALLOWED_SCRIPT_ENV_VARS` constant with safe vars: PATH, HOME, USER, LANG, LC_ALL, TZ
  - Created `_get_script_env(execution)` function to build safe environment dictionary
  - Modified `execute_script_step()` to use `_get_script_env()` instead of `**dict(os.environ)`
  - Added support for `MEDIC_ADDITIONAL_SCRIPT_ENV_VARS` env var to extend allowlist
  - Added 8 unit tests in `TestGetScriptEnvSecurity` class verifying:
    - MEDIC_SECRETS_KEY is NOT passed to scripts
    - DATABASE_URL is NOT passed to scripts
    - AWS credentials are NOT passed to scripts
    - Only allowlisted variables are passed
    - MEDIC context vars (MEDIC_EXECUTION_ID, MEDIC_PLAYBOOK_ID, MEDIC_SERVICE_ID) always present
    - MEDIC_ADDITIONAL_SCRIPT_ENV_VARS extends allowlist
    - Empty/whitespace handling for additional vars

- **Files changed**:
  - `Medic/Core/playbook_engine.py` (added ALLOWED_SCRIPT_ENV_VARS, _get_script_env, modified execute_script_step)
  - `tests/unit/test_playbook_engine.py` (new TestGetScriptEnvSecurity class with 8 tests)
  - `tasks/prd.json` (updated passes: true)

- **Learnings for future iterations**:
  - Environment variables passed to subprocess.run() via `env=` parameter replace the entire environment
  - Using `**dict(os.environ)` is a security antipattern - it leaks all secrets to subprocesses
  - Use `patch.dict('os.environ', {...})` for testing environment variables
  - The forward reference `"PlaybookExecution"` in type hints requires quotes for circular import prevention
  - Flake8 enforces 79 char line limit - keep comments under that limit

---
